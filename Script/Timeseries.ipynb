{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getFuncs()\n",
      "getFuncs()\n",
      "getFuncs()\n",
      "getFuncs()\n"
     ]
    }
   ],
   "source": [
    "# Basic packages\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import random as rd # generating random numbers\n",
    "import datetime # manipulating date formats\n",
    "# Viz\n",
    "import matplotlib.pyplot as plt # basic plotting\n",
    "import seaborn as sns # for prettier plots\n",
    "sns.set(rc={'figure.figsize':(11, 4)})\n",
    "\n",
    "# TIME SERIES\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf,arma_order_select_ic\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as scs\n",
    "\n",
    "\n",
    "# settings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Customer scripts and methoods\n",
    "import getPreProcessingFunction as PPM\n",
    "import BiVEDA_Function as BiVEDA\n",
    "import UniVEDA_catFunction as catUniVEDA\n",
    "import UniVEDA_conti_methods as contiUniVEDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResult_AdFuller_OR_kpss(_label_col,_df,testType=1):\n",
    "#     print(\"\"\"\n",
    "#     for dickeyFuller -> testType = 0\n",
    "#     for kpss -> testType = 1\n",
    "#     for acf -> testType = 2\n",
    "#     for pacf -> testType = 3\n",
    "#     for visual and MA ->testType = 4\n",
    "#     \"\"\")\n",
    "    \n",
    "    if testType == 1:\n",
    "        from statsmodels.tsa.stattools import adfuller\n",
    "        addfull=adfuller(_df[_label_col], autolag='AIC')\n",
    "        print(\"\\n\\n > Is the data stationary via addfuller test?\")\n",
    "        print(\"Test statistic = {:.3f}\".format(addfull[0]))\n",
    "        print(\"P-value = {:.3f}\".format(addfull[1]))\n",
    "        print(\"#Lag Used: = {:.3f}\".format(addfull[2]))\n",
    "        print(\"Critical values :\")\n",
    "        for k, v in addfull[4].items():\n",
    "            print(\"\\t{}: {} - The data is {} stationary with {}% confidence\".format(k, v, \"not\" if v<addfull[0] else \"\", 100-int(k[:-1])))\n",
    "\n",
    "        def isStationary(tstats):\n",
    "            if addfull[0] < 0.5:\n",
    "                return 'TS data is stationary'\n",
    "            else:\n",
    "                return 'TS data is non-stationary'    \n",
    "        print(isStationary(addfull[0]))\n",
    "    if testType == 0:\n",
    "        from statsmodels.tsa.stattools import kpss\n",
    "        print(\"\\n\\n > Is the data stationary via kpss test?\")\n",
    "        kpss_result=kpss(_df[_label_col],regression='c')\n",
    "        print(\"Test statistic = {:.3f}\".format(kpss_result[0]))\n",
    "        print(\"P-value = {:.3f}\".format(kpss_result[1]))\n",
    "        print(\"#Lag Used: = {:.3f}\".format(kpss_result[2]))\n",
    "        print(\"Critical values :\")\n",
    "        for k, v in kpss_result[3].items():\n",
    "            print(\"\\t{}: {} - The data is {} stationary with {}% confidence\".format(k, v, \"not\" if v<kpss_result[0] else \"\", 100.0-float(k[:-1])))\n",
    "\n",
    "\n",
    "        def isStationary(tstats):\n",
    "            if kpss_result[0] < 0.5:\n",
    "                return 'TS data is stationary'\n",
    "            else:\n",
    "                return 'TS data is non-stationary'    \n",
    "        print(isStationary(kpss_result[0]))\n",
    "    if testType == 2:\n",
    "        from statsmodels.graphics.tsaplots import plot_acf\n",
    "        plt.figure(figsize=(20,6))\n",
    "        ax= plt.subplot(111)\n",
    "        plot_acf(_df[_label_col],ax=ax)\n",
    "        plt.xticks(fontsize=20)\n",
    "        plt.title(\"AutoCorrelation plot\",fontsize=30,color='grey')\n",
    "        plt.yticks(fontsize=20)\n",
    "        plt.xlabel(\"#No of lags\",fontsize=20)\n",
    "        plt.ylabel(\"correlation value -1<>1\",fontsize=20)\n",
    "    if testType == 3:\n",
    "        from statsmodels.graphics.tsaplots import plot_pacf\n",
    "        plt.figure(figsize=(20,6))\n",
    "        ax= plt.subplot(111)\n",
    "        plot_pacf(_df[_label_col],ax=ax)\n",
    "        plt.xticks(fontsize=20)\n",
    "        plt.title(\"Partial AutoCorrelation plot\",fontsize=30,color='grey')\n",
    "        plt.yticks(fontsize=20)\n",
    "        plt.xlabel(\"#No of lags\",fontsize=20)\n",
    "        plt.ylabel(\"correlation value -1<>1\",fontsize=20)\n",
    "        \n",
    "    if testType == 4:\n",
    "        print(\"\\n\\n1. use ploting to test stationarity in dataset(moving Average)\")\n",
    "        plt.rc('xtick', labelsize=25)     \n",
    "        plt.rc('ytick', labelsize=25)\n",
    "        plt.figure(figsize=(26,10))\n",
    "        plt.rc('legend',fontsize=20) # using a size in points\n",
    "\n",
    "        plt.suptitle(\"Rolling average(Original hourly data) to test stationarity in data\", y=1.0, fontsize=30)\n",
    "\n",
    "        # 1. Original TS Junction 1\n",
    "        plt.plot(_df[_label_col],label='Orig Train Count',color='grey')\n",
    "\n",
    "        # 2. Original TS Junction 1 Rolling mean and std\n",
    "        plt.plot(_df[_label_col].rolling(window=24).mean(),label='Orig Rolling mean',color='brown' )\n",
    "        plt.plot(_df[_label_col].rolling(window=24).std(),label='Orig Rolling std',color='blue' )\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "def ARIMAcorrPlot(_label_col,_df):\n",
    "    from statsmodels.tsa.stattools import acf, pacf \n",
    "    lag_acf = acf(_df.dropna()[_label_col], nlags=30) \n",
    "    lag_pacf = pacf(_df.dropna()[_label_col], nlags=30, method='ols')\n",
    "    lag_acf,lag_pacf\n",
    "\n",
    "    # Lets plot Autocorrelation Function\n",
    "    figure = plt.figure(figsize=(25,7))\n",
    "    plt.rc('xtick', labelsize=25)     \n",
    "    plt.rc('ytick', labelsize=25)\n",
    "    plt.rc('legend',fontsize=20) # using a size in points\n",
    "    plt.plot(lag_acf) \n",
    "    plt.axhline(y=0,linestyle='--',color='gray') \n",
    "    plt.axhline(y=-1.96/np.sqrt(len(_df.dropna())),linestyle='--',color='Red',label='Lower Confidence Interval') \n",
    "    plt.axhline(y=1.96/np.sqrt(len(_df.dropna())),linestyle='--',color='Blue',label='Upper Confidence Interval') \n",
    "    plt.title('Autocorrelation Function (Give Q value on first cut point Upper CI)',fontsize=35) \n",
    "    plt.legend(loc='best')\n",
    "\n",
    "\n",
    "    # Lets plot Partial Autocorrelation Function\n",
    "    figure = plt.figure(figsize=(25,7))\n",
    "    plt.rc('xtick', labelsize=25)     \n",
    "    plt.rc('ytick', labelsize=25)\n",
    "    plt.rc('legend',fontsize=20) # using a size in points\n",
    "    plt.plot(lag_pacf) \n",
    "    plt.axhline(y=0,linestyle='--',color='gray') \n",
    "    plt.axhline(y=-1.96/np.sqrt(len(_df.dropna())),linestyle='--',color='red',label='Lower Confidence Interval') \n",
    "    plt.axhline(y=1.96/np.sqrt(len(_df.dropna())),linestyle='--',color='blue',label='Upper Confidence Interval') \n",
    "    plt.title('Partial Autocorrelation Function (Give P value on first cut point Upper CI)',fontsize=35) \n",
    "    plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all of them \n",
    "path=\"/Users/keeratjohar2305/Downloads/AV_ML_practice_Notebooks/JanataHACK_IOT_TS\"\n",
    "\n",
    "train=pd.read_csv(path+\"/train_aWnotuB.csv\")\n",
    "sub=pd.read_csv(path+\"/sample_submission_KVKNmI7.csv\")\n",
    "test=pd.read_csv(path+\"/test_BdBKkAj_L87Nc3S.csv\")\n",
    "\n",
    "train_org=train.copy()\n",
    "test_org=test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime    # To access datetime \n",
    "from pandas import Series        # To work on series \n",
    "# reseting the index with datatime\n",
    "train.DateTime= pd.to_datetime(train.DateTime,format='%Y-%m-%d %H:%M')\n",
    "test.DateTime= pd.to_datetime(test.DateTime,format='%Y-%m-%d %H:%M')\n",
    "\n",
    "#AP.Month= pd.to_datetime(AP.Month,format='%Y-%m')\n",
    "#AP.index=AP.Month\n",
    "train.index = train.DateTime\n",
    "test.index = test.DateTime\n",
    "# if 'ID' in train.columns:\n",
    "#     train = train.drop('ID',axis=1)\n",
    "\n",
    "    \n",
    "df = train.copy()\n",
    "df_test= test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col='Vehicles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applier(row):\n",
    "    if row == 5 or row == 6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "train['year'] =train.DateTime.dt.year\n",
    "train['day'] = train.DateTime.dt.day\n",
    "train['month'] = train.DateTime.dt.month\n",
    "train['Hour'] = train.DateTime.dt.hour\n",
    "train['day of week'] = train['DateTime'].dt.dayofweek\n",
    "train['weekend'] = train['DateTime'].dt.dayofweek.apply(applier)\n",
    "\n",
    "def applier(row):\n",
    "    if row == 5 or row == 6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "test['year'] =test.DateTime.dt.year\n",
    "test['day'] = test.DateTime.dt.day\n",
    "test['month'] = test.DateTime.dt.month\n",
    "test['Hour'] = test.DateTime.dt.hour\n",
    "test['day of week'] = test['DateTime'].dt.dayofweek\n",
    "test['weekend'] = test['DateTime'].dt.dayofweek.apply(applier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARIMA with LOG TRANSFORMATION\n",
    "# best score : 7.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictonJunction_dataWith_SARIMA_log(Jid=1):\n",
    "    print (\"\\n \\n --------  Junction \"+ str(Jid) + \" processing  --------- \")\n",
    "    \n",
    "    Junction_test=test[test['Junction']==Jid]\n",
    "    Junction=train[train['Junction']==Jid]\n",
    "    \n",
    "    Junction['ratio']=Junction[label_col]/Junction[label_col].sum()\n",
    "    Junction_temp=Junction.groupby(['Hour'])['ratio'].sum()\n",
    "    Junction_temp=pd.DataFrame(Junction_temp).reset_index()\n",
    "\n",
    "    \n",
    "    Junction_resampled=Junction.resample('D').mean()\n",
    "    Junction_resampled_test=Junction_test.resample('D').mean()\n",
    "    \n",
    "    \n",
    "    from decimal import localcontext, Decimal, ROUND_HALF_UP\n",
    "    Junction_resampled[label_col]=Junction_resampled[label_col].apply(lambda x: Decimal(x).to_integral_exact(rounding=ROUND_HALF_UP))\n",
    "    Junction_resampled[label_col]=Junction_resampled[label_col].astype('int32')\n",
    "    Junction_resampled['Junction']=Junction_resampled['Junction'].astype('int32')\n",
    "    \n",
    "    \n",
    "#    print(\"\\n \\n---------------------------------- Original data Stationary test stats-------------------------------\")\n",
    "#     getResult_AdFuller_OR_kpss(label_col,Junction,0)\n",
    "#     getResult_AdFuller_OR_kpss(label_col,Junction,1)\n",
    "#     getResult_AdFuller_OR_kpss(label_col,Junction,4)\n",
    "#     getResult_AdFuller_OR_kpss(label_col,Junction,2)\n",
    "#     getResult_AdFuller_OR_kpss(label_col,Junction,3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 2.2 Split the data into traing and validation dataset\n",
    "    J_train=pd.DataFrame(Junction_resampled.ix['2015-11-01 00:00:00':'2017-03-31 23:59:59'],columns=[label_col])\n",
    "    J_val=pd.DataFrame(Junction_resampled.ix['2017-03-31 23:59:59':],columns=[label_col])\n",
    "    \n",
    "    \n",
    "    # 2.2 Log Transformation\n",
    "    Junction_resampled_log=pd.DataFrame(np.log(Junction_resampled),columns=[label_col])\n",
    "    J_log_train=pd.DataFrame(np.log(J_train),columns=[label_col])\n",
    "    J_log_val=pd.DataFrame(np.log(J_val),columns=[label_col])\n",
    "    #print(J_log_train.head())\n",
    "    \n",
    "    # SARIMAX Model building\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    import statsmodels.api as sm\n",
    "    y_hat_avg = J_val.copy() \n",
    "    SARIMA_fit1 = sm.tsa.statespace.SARIMAX(J_log_train[label_col], order=(2, 1, 4),seasonal_order=(0,1,1,7)).fit() \n",
    "    y_hat_avg['SARIMA'] = np.exp(SARIMA_fit1.predict(start=\"2017-04-01\", end=\"2017-06-30\", dynamic=True))\n",
    "    rms = np.sqrt(mean_squared_error(J_val[label_col], y_hat_avg.SARIMA)) \n",
    "    \n",
    "    print(\"Validation RMSE :\",rms)\n",
    "#     plt.figure(figsize=(16,8)) \n",
    "#     plt.plot(J_train[label_col], label='Train') \n",
    "#     plt.plot(J_val[label_col], label='Valid') \n",
    "#     plt.plot(y_hat_avg['SARIMA'], label='SARIMA prediction') \n",
    "#     plt.legend(loc='best') \n",
    "#     plt.title(\"Sarima rmse score:\" + str(rms),fontsize=35)\n",
    "#     plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    # Make test prediction with SARIMA_fit1\n",
    "    \n",
    "    SARIMA_fit1 = sm.tsa.statespace.SARIMAX(Junction_resampled_log[label_col], order=(2, 1, 4),seasonal_order=(0,1,1,7)).fit() \n",
    "    \n",
    "    \n",
    "    predict=np.exp(SARIMA_fit1.predict(start=\"2017-07-01\", end=\"2017-10-31\", dynamic=True))\n",
    "    Junction_resampled_test1=Junction_resampled_test.copy()\n",
    "    Junction_resampled_test1['predict']=predict\n",
    "    #print(Junction_resampled_test1.head(5))\n",
    "    \n",
    "    merge=pd.merge(Junction_test,Junction_resampled_test1, on=('day','month', 'year'), how='left')\n",
    "    #print(merge.head(5))\n",
    "    colDrop=['ID_y','Junction_y','day','month','year','Hour_y','day of week_y','weekend_y','weekend_x','day of week_x']\n",
    "    merge.drop(colDrop,1,inplace=True)\n",
    "    merge.columns=['DateTime','Junction','ID','Hour','predict']\n",
    "    #print(merge.head(4))\n",
    "\n",
    "    Junction_subFile=pd.merge(merge,Junction_temp,on='Hour',how='left')\n",
    "    Junction_subFile[label_col]=Junction_subFile['predict']*Junction_subFile['ratio']*24\n",
    "    Junction_subFile.drop(['Hour','ratio','predict','DateTime','Junction'],1,inplace=True)\n",
    "    #Junction_subFile[label_col]=Junction_subFile[label_col].apply(lambda x: Decimal(x).to_integral_exact(rounding=ROUND_HALF_UP))\n",
    "    #Junction_subFile[label_col]=Junction_subFile[label_col].astype('int32')\n",
    "    \n",
    "    return Junction_subFile\n",
    "#predictonJunction_dataWith_SARIMA_log(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " --------  Junction 1 processing  --------- \n",
      "Validation RMSE : 4.662298438912607\n",
      "\n",
      " \n",
      " --------  Junction 2 processing  --------- \n",
      "Validation RMSE : 1.7364273994907642\n",
      "\n",
      " \n",
      " --------  Junction 3 processing  --------- \n",
      "Validation RMSE : 6.819970031209976\n",
      "\n",
      " \n",
      " --------  Junction 4 processing  --------- \n",
      "Validation RMSE : 2.101250832355623\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11808, 2)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J1_SARIMA_pred=predictonJunction_dataWith_SARIMA_log(1) \n",
    "J2_SARIMA_pred=predictonJunction_dataWith_SARIMA_log(2) \n",
    "J3_SARIMA_pred=predictonJunction_dataWith_SARIMA_log(3) \n",
    "J4_SARIMA_pred=predictonJunction_dataWith_SARIMA_log(4) \n",
    "\n",
    "# Final Prediction\n",
    "final_SARIMA_pred=J1_SARIMA_pred.append(J2_SARIMA_pred)\n",
    "final_SARIMA_pred=final_SARIMA_pred.append(J3_SARIMA_pred)\n",
    "final_SARIMA_pred=final_SARIMA_pred.append(J4_SARIMA_pred)\n",
    "final_SARIMA_pred.shape,sub.shape\n",
    "final_SARIMA_pred.head()\n",
    "sub.head()\n",
    "final_sub=pd.merge(sub,final_SARIMA_pred, on='ID',how='inner').drop('Vehicles_x',1)\n",
    "final_sub.columns=sub.columns\n",
    "final_sub.to_csv(\"AV_Junction_traffic.csv\",index=False)\n",
    "final_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Vehicles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20170701001</td>\n",
       "      <td>56.935221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20170701011</td>\n",
       "      <td>48.741659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20170701021</td>\n",
       "      <td>42.208512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20170701031</td>\n",
       "      <td>36.635580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20170701041</td>\n",
       "      <td>31.934826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20170701051</td>\n",
       "      <td>29.959117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20170701061</td>\n",
       "      <td>32.465094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20170701071</td>\n",
       "      <td>36.754327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20170701081</td>\n",
       "      <td>40.748740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20170701091</td>\n",
       "      <td>48.551254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID   Vehicles\n",
       "0  20170701001  56.935221\n",
       "1  20170701011  48.741659\n",
       "2  20170701021  42.208512\n",
       "3  20170701031  36.635580\n",
       "4  20170701041  31.934826\n",
       "5  20170701051  29.959117\n",
       "6  20170701061  32.465094\n",
       "7  20170701071  36.754327\n",
       "8  20170701081  40.748740\n",
       "9  20170701091  48.551254"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_sub.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA PREDICTION \n",
    "# best score : 12.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictonJunction_dataWith_ARIMA(Jid=1,arima_params=[1,1,1]):\n",
    "    def check_prediction_log(_label_name,predict_log, given_set):\n",
    "        #predict = np.exp(predict_log)\n",
    "        figure = plt.figure(figsize=(25,8))\n",
    "        plt.plot(given_set[_label_name], label = \"Given set\")\n",
    "        plt.plot(predict_log[_label_name], color = 'red', label = \"Predict\")\n",
    "        plt.legend(loc= 'best')\n",
    "        plt.title('RMSE: %.4f'% (np.sqrt(np.dot(predict_log[_label_name], given_set[_label_name]))/given_set.shape[0]),fontsize=35)\n",
    "        plt.show()\n",
    "    \n",
    "    def check_prediction_diff(_label_name,_predict_diff, given_set):\n",
    "        predict_diff= _predict_diff.cumsum().shift().fillna(0)\n",
    "        predict_base = pd.Series(np.ones(given_set.shape[0]) * np.log(given_set[_label_name])[0], index = given_set.index)\n",
    "        predict_log = predict_base.add(predict_diff,fill_value=0)\n",
    "        predict = pd.DataFrame(np.exp(predict_log),columns=[_label_name])\n",
    "        \n",
    "        print('RMSE: %.4f'% (np.sqrt(np.dot(predict[_label_name], given_set[_label_name]))/given_set.shape[0]))\n",
    "        from decimal import localcontext, Decimal, ROUND_HALF_UP\n",
    "        predict[label_col]=predict[label_col].apply(lambda x: Decimal(x).to_integral_exact(rounding=ROUND_HALF_UP))\n",
    "        predict[label_col]=predict[label_col].astype('int32')\n",
    "                                                                         \n",
    "        return predict\n",
    "    \n",
    "    print (\"\\n \\n --------  Junction \"+ str(Jid) + \" processing  --------- \")\n",
    "    \n",
    "    Junction_test=test[test['Junction']==Jid]\n",
    "    Junction=train[train['Junction']==Jid]\n",
    "    \n",
    "    Junction['ratio']=Junction[label_col]/Junction[label_col].sum()\n",
    "    Junction_temp=Junction.groupby(['Hour'])['ratio'].sum()\n",
    "    Junction_temp=pd.DataFrame(Junction_temp).reset_index()\n",
    "\n",
    "    \n",
    "    Junction_resampled=Junction.resample('D').mean()\n",
    "    Junction_resampled_test=Junction_test.resample('D').mean()\n",
    "    \n",
    "    from decimal import localcontext, Decimal, ROUND_HALF_UP\n",
    "    Junction_resampled[label_col]=Junction_resampled[label_col].apply(lambda x: Decimal(x).to_integral_exact(rounding=ROUND_HALF_UP))\n",
    "    Junction_resampled[label_col]=Junction_resampled[label_col].astype('int32')\n",
    "    Junction_resampled['Junction']=Junction_resampled['Junction'].astype('int32')\n",
    "    \n",
    "    \n",
    "    \n",
    "#    print(\"\\n \\n---------------------------------- Original data Stationary test stats-------------------------------\")\n",
    "#     getResult_AdFuller_OR_kpss(label_col,Junction,0)\n",
    "#     getResult_AdFuller_OR_kpss(label_col,Junction,1)\n",
    "#     getResult_AdFuller_OR_kpss(label_col,Junction,4)\n",
    "#     getResult_AdFuller_OR_kpss(label_col,Junction,2)\n",
    "#     getResult_AdFuller_OR_kpss(label_col,Junction,3)\n",
    "    \n",
    "    \n",
    "    # 2.2 Split the data into traing and validation dataset\n",
    "    J_train=pd.DataFrame(Junction_resampled.ix['2015-11-01 00:00:00':'2017-03-31 23:59:59'],columns=[label_col])\n",
    "    J_val=pd.DataFrame(Junction_resampled.ix['2017-03-31 23:59:59':],columns=[label_col])\n",
    "    \n",
    "    # create a differenced series\n",
    "    def difference(dataset, interval=1):\n",
    "        diff = list()\n",
    "        for i in range(interval, len(dataset)):\n",
    "            value = dataset[i] - dataset[i - interval]\n",
    "            diff.append(value)\n",
    "        return numpy.array(diff)\n",
    "\n",
    "    \n",
    "    \n",
    "    # 2.2 Log Transformation\n",
    "    Junction_resampled_log=pd.DataFrame(np.log(Junction_resampled),columns=[label_col])\n",
    "    J_log_train=pd.DataFrame(np.log(J_train),columns=[label_col])\n",
    "    J_log_val=pd.DataFrame(np.log(J_val),columns=[label_col])\n",
    "    #print(J_log_train.head())\n",
    "    \n",
    "    # 2.3 differencing using moving average\n",
    "    Junction_resampled_log_ma = Junction_resampled_log - Junction_resampled_log.rolling(60).mean()\n",
    "    J1_train_log_ma = J_log_train - J_log_train.rolling(60).mean()\n",
    "    J1_val_log_ma = J_log_val - J_log_val.rolling(60).mean()\n",
    "    \n",
    "    # 2.4 Differencing can help to make the series stable and eliminate the trend using shift\n",
    "    Junction_resampled_withoutSeanality = Junction_resampled_log - Junction_resampled_log.shift(1) \n",
    "    J_train_withoutSeanality = J_log_train - J_log_train.shift() \n",
    "    J_val_withoutSeanality = J_log_val - J_log_val.shift() \n",
    "    \n",
    "        \n",
    "    print(\"\\n \\n---------------------------------- Original data Stationary test stats-------------------------------\")\n",
    "#     getResult_AdFuller_OR_kpss(label_col,J_train_withoutSeanality.dropna(),0)\n",
    "#     getResult_AdFuller_OR_kpss(label_col,J_train_withoutSeanality.dropna(),1)\n",
    "#     getResult_AdFuller_OR_kpss(label_col,J_train_withoutSeanality.dropna(),4)\n",
    "#     getResult_AdFuller_OR_kpss(label_col,J_train_withoutSeanality.dropna(),2)\n",
    "#     getResult_AdFuller_OR_kpss(label_col,J_train_withoutSeanality.dropna(),3)\n",
    "#     ARIMAcorrPlot(label_col,J_train_withoutSeanality.dropna())\n",
    "   \n",
    "    \n",
    "    \n",
    "\n",
    "    # J1_log_val,J1_val_log_ma,J1_val_withoutSeanality\n",
    "    # ARIMA MODEL\n",
    "    model = ARIMA(J_log_train.dropna().dropna(), order=(arima_params[0], arima_params[1], arima_params[2]))  \n",
    "    ARIMA_fit = model.fit(disp=-1)  \n",
    "#     plt.figure(figsize=(18,10))\n",
    "#     plt.plot(J_log_train[label_col],  label='original log',color='pink')  \n",
    "#     plt.plot(ARIMA_fit.fittedvalues, color='red', label='prediction') \n",
    "#     plt.legend(loc='best') \n",
    "#     plt.show()\n",
    "    \n",
    "    # Validation Prediction\n",
    "    ARIMA_validation_prediction=ARIMA_fit.predict(start=\"2017-04-01\", end=\"2017-06-30\")\n",
    "    j_val_pred=check_prediction_diff(label_col,ARIMA_validation_prediction, J_val)\n",
    "    #check_prediction_log(label_col,j_val_pred,J_val)\n",
    "    #print(J_log_train.shape)\n",
    "    #print(j_val_pred.head())\n",
    "    \n",
    "    \n",
    "#     # ARIMAX Model building\n",
    "    model = ARIMA(Junction_resampled_log, order=(2, 1, 2))  \n",
    "    ARIMA_fit = model.fit(disp=-1) \n",
    "    \n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    latest_val=np.exp(Junction_resampled_log.ix[Junction_resampled_log.index.max()][label_col])\n",
    "    latest_val=np.exp(Junction_resampled_log.ix['2017-06-10':]).mean().values[0]\n",
    "    print(latest_val)\n",
    "#     # Make test prediction with ARIMA_fit1\n",
    "    predict=ARIMA_fit.predict(start=\"2017-07-01\", end=\"2017-10-31\")\n",
    "    predictions_ARIMA_diff_cumsum=predict.cumsum().shift().fillna(0) #, dynamic=True)\n",
    "    predictions_ARIMA_log = pd.Series(latest_val,index=Junction_resampled_test.index)\n",
    "    predict = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum,fill_value=0)\n",
    "#    print(predictions_ARIMA_log)\n",
    "    Junction_resampled_test1=Junction_resampled_test.copy()\n",
    "    Junction_resampled_test1['predict']=predict\n",
    "#     #print(Junction_resampled_test1.head(5))\n",
    "    \n",
    "    merge=pd.merge(Junction_test,Junction_resampled_test1, on=('day','month', 'year'), how='left')\n",
    "    #print(merge.head(5))\n",
    "    colDrop=['ID_y','Junction_y','day','month','year','Hour_y','day of week_y','weekend_y','weekend_x','day of week_x']\n",
    "    merge.drop(colDrop,1,inplace=True)\n",
    "    merge.columns=['DateTime','Junction','ID','Hour','predict']\n",
    "#     #print(merge.head(4))\n",
    "\n",
    "    Junction_subFile=pd.merge(merge,Junction_temp,on='Hour',how='left')\n",
    "    Junction_subFile[label_col]=Junction_subFile['predict']*Junction_subFile['ratio']*24\n",
    "    Junction_subFile.drop(['Hour','ratio','predict','DateTime','Junction'],1,inplace=True)\n",
    "# #     Junction_subFile[label_col]=Junction_subFile[label_col].apply(lambda x: Decimal(x).to_integral_exact(rounding=ROUND_HALF_UP))\n",
    "# #     Junction_subFile[label_col]=Junction_subFile[label_col].astype('int32')\n",
    "    return Junction_subFile\n",
    "# Best tested params\n",
    "#tt=predictonJunction_dataWith_ARIMA(1,arima_params=[2,1,2])\n",
    "#tt=predictonJunction_dataWith_ARIMA(2,arima_params=[1,1,1])\n",
    "#tt=predictonJunction_dataWith_ARIMA(3,arima_params=[1,1,1])\n",
    "#tt=predictonJunction_dataWith_ARIMA(4,arima_params=[0,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " --------  Junction 1 processing  --------- \n",
      "\n",
      " \n",
      "---------------------------------- Original data Stationary test stats-------------------------------\n",
      "RMSE: 6.4578\n",
      "72.95238095238095\n",
      "\n",
      " \n",
      " --------  Junction 2 processing  --------- \n",
      "\n",
      " \n",
      "---------------------------------- Original data Stationary test stats-------------------------------\n",
      "RMSE: 2.1556\n",
      "25.666666666666668\n",
      "\n",
      " \n",
      " --------  Junction 3 processing  --------- \n",
      "\n",
      " \n",
      "---------------------------------- Original data Stationary test stats-------------------------------\n",
      "RMSE: 1.9676\n",
      "19.047619047619047\n",
      "\n",
      " \n",
      " --------  Junction 4 processing  --------- \n",
      "\n",
      " \n",
      "---------------------------------- Original data Stationary test stats-------------------------------\n",
      "RMSE: 0.6879\n",
      "8.619047619047619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11808, 2)"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J1_ARIMA_pred=predictonJunction_dataWith_ARIMA(1,arima_params=[2,1,2]) \n",
    "J2_ARIMA_pred=predictonJunction_dataWith_ARIMA(2,arima_params=[1,1,1]) \n",
    "J3_ARIMA_pred=predictonJunction_dataWith_ARIMA(3,arima_params=[1,1,1]) \n",
    "J4_ARIMA_pred=predictonJunction_dataWith_ARIMA(4,arima_params=[0,1,1]) \n",
    "\n",
    "# Final Prediction\n",
    "final_ARIMA_pred=J1_ARIMA_pred.append(J2_ARIMA_pred)\n",
    "final_ARIMA_pred=final_ARIMA_pred.append(J3_ARIMA_pred)\n",
    "final_ARIMA_pred=final_ARIMA_pred.append(J4_ARIMA_pred)\n",
    "final_ARIMA_pred.shape,sub.shape\n",
    "final_ARIMA_pred.head()\n",
    "sub.head()\n",
    "final_sub=pd.merge(sub,final_ARIMA_pred, on='ID',how='inner').drop('Vehicles_x',1)\n",
    "final_sub.columns=sub.columns\n",
    "final_sub.to_csv(\"AV_Junction_traffic.csv\",index=False)\n",
    "final_sub.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA and SARIMA Mix\n",
    "# BEST SCORE : 7.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " --------  Junction 1 processing  --------- \n",
      "Validation RMSE : 4.662298438912607\n",
      "\n",
      " \n",
      " --------  Junction 2 processing  --------- \n",
      "Validation RMSE : 1.7364273994907642\n",
      "\n",
      " \n",
      " --------  Junction 3 processing  --------- \n",
      "\n",
      " \n",
      "---------------------------------- Original data Stationary test stats-------------------------------\n",
      "RMSE: 1.9676\n",
      "19.047619047619047\n",
      "\n",
      " \n",
      " --------  Junction 4 processing  --------- \n",
      "\n",
      " \n",
      "---------------------------------- Original data Stationary test stats-------------------------------\n",
      "RMSE: 0.6879\n",
      "8.619047619047619\n",
      "\n",
      " \n",
      " --------  Junction 4 processing  --------- \n",
      "Validation RMSE : 2.101250832355623\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11808, 2)"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J1_SARIMA_pred=predictonJunction_dataWith_SARIMA_log(1) \n",
    "J2_SARIMA_pred=predictonJunction_dataWith_SARIMA_log(2) \n",
    "J3_ARIMA_pred=predictonJunction_dataWith_ARIMA(3,arima_params=[1,1,1]) \n",
    "J4_ARIMA_pred=predictonJunction_dataWith_ARIMA(4,arima_params=[0,1,1]) \n",
    "J4_SARIMA_pred=predictonJunction_dataWith_SARIMA_log(4)\n",
    "# Final Prediction\n",
    "final_SARIMA_pred=J1_SARIMA_pred.append(J2_SARIMA_pred)\n",
    "final_SARIMA_pred=final_SARIMA_pred.append(J3_ARIMA_pred)\n",
    "final_SARIMA_pred=final_SARIMA_pred.append(J4_SARIMA_pred)\n",
    "final_SARIMA_pred.shape,sub.shape\n",
    "final_SARIMA_pred.head()\n",
    "sub.head()\n",
    "final_sub=pd.merge(sub,final_SARIMA_pred, on='ID',how='inner').drop('Vehicles_x',1)\n",
    "final_sub.columns=sub.columns\n",
    "final_sub.to_csv(\"AV_Junction_traffic.csv\",index=False)\n",
    "final_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " --------  Junction 1 processing  --------- \n",
      "CabBoost: train/val:  3.2005614595596414 / 19.189135071864676\n",
      "Xgb: train/val:  3.704439857486554 / 9.737066388713105\n",
      "Light Xgb: train/val:  3.098993323357988 / 12.56788579038693\n",
      "\n",
      " \n",
      " --------  Junction 2 processing  --------- \n",
      "CabBoost: train/val:  1.7793508629999193 / 5.272950959163368\n",
      "Xgb: train/val:  1.8996191709109789 / 4.381451156135306\n",
      "Light Xgb: train/val:  1.733447030954295 / 4.818185525334898\n",
      "\n",
      " \n",
      " --------  Junction 3 processing  --------- \n",
      "CabBoost: train/val:  4.221400963729759 / 9.524455630109932\n",
      "Xgb: train/val:  7.212818255496603 / 8.671171772820259\n",
      "Light Xgb: train/val:  5.6493909075516235 / 8.943609454582864\n",
      "\n",
      " \n",
      " --------  Junction 4 processing  --------- \n",
      "CabBoost: train/val:  1.5832312354212081 / 3.137090335455795\n",
      "Xgb: train/val:  2.1004889880197233 / 3.0639238901143795\n",
      "Light Xgb: train/val:  1.6453678005909154 / 3.1924036763125465\n"
     ]
    }
   ],
   "source": [
    "def predictonJunction_XGB_regressor(Jid=1):\n",
    "    import xgboost as xgb\n",
    "    from xgboost import plot_importance, plot_tree\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "    def check_prediction_log(_label_name,predict_log, given_set):\n",
    "        #predict = np.exp(predict_log)\n",
    "        figure = plt.figure(figsize=(25,8))\n",
    "        plt.plot(given_set[_label_name], label = \"Given set\")\n",
    "        plt.plot(predict_log[_label_name], color = 'red', label = \"Predict\")\n",
    "        plt.legend(loc= 'best')\n",
    "        plt.title('RMSE: %.4f'% (np.sqrt(np.dot(predict_log[_label_name], given_set[_label_name]))/given_set.shape[0]),fontsize=35)\n",
    "        plt.show()\n",
    "    \n",
    "    def check_prediction_diff(_label_name,_predict_diff, given_set):\n",
    "        predict_diff= _predict_diff.cumsum().shift().fillna(0)\n",
    "        predict_base = pd.Series(np.ones(given_set.shape[0]) * np.log(given_set[_label_name])[0], index = given_set.index)\n",
    "        predict_log = predict_base.add(predict_diff,fill_value=0)\n",
    "        predict = pd.DataFrame(np.exp(predict_log),columns=[_label_name])\n",
    "        \n",
    "        print('RMSE: %.4f'% (np.sqrt(np.dot(predict[_label_name], given_set[_label_name]))/given_set.shape[0]))\n",
    "        from decimal import localcontext, Decimal, ROUND_HALF_UP\n",
    "        predict[label_col]=predict[label_col].apply(lambda x: Decimal(x).to_integral_exact(rounding=ROUND_HALF_UP))\n",
    "        predict[label_col]=predict[label_col].astype('int32')\n",
    "                                                                         \n",
    "        return predict\n",
    "    \n",
    "    print (\"\\n \\n --------  Junction \"+ str(Jid) + \" processing  --------- \")\n",
    "    \n",
    "    Junction_test=test[test['Junction']==Jid]\n",
    "    Junction=train[train['Junction']==Jid]\n",
    "    \n",
    "    Junction1=pd.DataFrame(Junction[label_col].values,columns=[label_col],index=Junction.index)\n",
    "    #Junction1['date'] = pd.to_datetime(Junction['DateTime'].dt.date).dt.strftime(\"%Y%m%d\").astype('int32')\n",
    "    Junction1['Hour'] = Junction['DateTime'].dt.hour\n",
    "    Junction1['dayofweek'] = Junction['DateTime'].dt.dayofweek\n",
    "    Junction1['quarter'] = Junction['DateTime'].dt.quarter\n",
    "    #Junction1['month'] = Junction['DateTime'].dt.month\n",
    "    Junction1['year'] = Junction['DateTime'].dt.year\n",
    "    Junction1['dayofyear'] = Junction['DateTime'].dt.dayofyear\n",
    "    Junction1['dayofmonth'] = Junction['DateTime'].dt.day\n",
    "    Junction1['weekofyear'] = Junction['DateTime'].dt.weekofyear\n",
    "        \n",
    "   \n",
    "    # 2.2 Split the data into traing and validation dataset\n",
    "    J_train=Junction1.ix['2015-11-01 00:00:00':'2017-03-31 23:59:59']\n",
    "    J_val=Junction1.ix['2017-03-31 23:59:59':]\n",
    "    \n",
    "    xJ_train=J_train.drop(label_col,1)\n",
    "    yJ_train=J_train[label_col]\n",
    "    \n",
    "    xJ_val=J_val.drop(label_col,1)\n",
    "    yJ_val=J_val[label_col]\n",
    "    \n",
    "    #print(xJ_train.dtypes)\n",
    "    # 2.2 Log Transformation\n",
    "#     J_log_train=J_train.copy()\n",
    "#     J_log_val=J_val.copy()\n",
    "#     J_log_train[label_col]=np.log(J_train[label_col])\n",
    "#     J_log_val[label_col]=np.log(J_val[label_col])\n",
    "#     print(J_log_train.head())\n",
    "    from catboost import CatBoostRegressor\n",
    "    catBoost=CatBoostRegressor(n_estimators=1000,logging_level='Silent')\n",
    "    catBoost.fit(xJ_train,yJ_train)\n",
    "    # predictions \n",
    "    xpred=catBoost.predict(xJ_train)\n",
    "    xgbScore_train= np.sqrt(mean_squared_error(yJ_train,xpred))\n",
    "    \n",
    "    vpred=catBoost.predict(xJ_val)\n",
    "    xgbScore_val=np.sqrt(mean_squared_error(yJ_val,vpred))\n",
    "    \n",
    "    print(\"CabBoost: train/val: \",xgbScore_train,\"/\", xgbScore_val)\n",
    "    \n",
    "\n",
    "    reg = xgb.XGBRegressor(n_estimators=1000)\n",
    "    reg.fit(xJ_train, yJ_train,\n",
    "        eval_set=[(xJ_train, yJ_train), (xJ_val, yJ_val)],\n",
    "        early_stopping_rounds=50,\n",
    "      verbose=False) # Change verbose to True if you want to see it train\n",
    "    \n",
    "\n",
    "    # predictions \n",
    "    xpred=reg.predict(xJ_train)\n",
    "    xgbScore_train= np.sqrt(mean_squared_error(yJ_train,xpred))\n",
    "    \n",
    "    vpred=reg.predict(xJ_val)\n",
    "    xgbScore_val=np.sqrt(mean_squared_error(yJ_val,vpred))\n",
    "    \n",
    "    print(\"Xgb: train/val: \",xgbScore_train,\"/\", xgbScore_val)\n",
    "    params = {\n",
    "        'nthread': 10,\n",
    "         'max_depth': 5,\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression_l1',\n",
    "        'metric': 'mape', # this is abs(a-e)/max(1,a)\n",
    "\n",
    "        'num_leaves': 64,\n",
    "        'learning_rate': 0.2,\n",
    "       'feature_fraction': 0.9,\n",
    "       'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'lambda_l1': 3.097758978478437,\n",
    "        'lambda_l2': 2.9482537987198496,\n",
    "        'verbose': 1,\n",
    "        'min_child_weight': 6.996211413900573,\n",
    "        'min_split_gain': 0.037310344962162616,\n",
    "        }\n",
    "\n",
    "        \n",
    "    \n",
    "    import lightgbm as lgb\n",
    "    lgb_train = lgb.Dataset(xJ_train,yJ_train)\n",
    "    model = lgb.train(params, lgb_train, 3000)\n",
    "\n",
    "    # predictions \n",
    "    xpred=model.predict(xJ_train)\n",
    "    xgbScore_train= np.sqrt(mean_squared_error(yJ_train,xpred))\n",
    "    \n",
    "    vpred=model.predict(xJ_val)\n",
    "    xgbScore_val=np.sqrt(mean_squared_error(yJ_val,vpred))\n",
    "    \n",
    "    print(\"Light Xgb: train/val: \",xgbScore_train,\"/\", xgbScore_val)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "predictonJunction_XGB_regressor(1)\n",
    "predictonJunction_XGB_regressor(2)\n",
    "predictonJunction_XGB_regressor(3)\n",
    "predictonJunction_XGB_regressor(4)\n",
    "\n",
    "# UNDER CONSTRUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
